import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from sklearn import metrics
from scipy.stats import zscore
from sklearn.model_selection import KFold


idx=10000
f = open("/kaggle/input/ubiquant-market-prediction/train.csv",'r')
count=0

train_data=[]

while True:
  line=f.readline() # 먼저, 오픈한 f를 한줄한줄 읽어온다

    
  if 0 < count < idx+1:  # 각 열에 대한 설명인 첫행은 스킵하기 위해 0 초과, 원하는 데이터 개수의 70퍼센트만큼 train data로
    append_data=line.split(",") # 불러온 행의 요소들을 ',' 단위로 스플릿해준다. (csv 파일이기 때문)
    append_data=list(map(float,append_data)) # 스플릿해준 각 요소들이 스트링 값이라 이를 float로 변환
    
    train_data.append(append_data) # 만들어낸 append data를 train_data에 다시 넣어주면 이중리스트 형성(매 루프마다 리스트가 한개씩 들어가는 꼴)
                                   # 이렇게 만들어낸 train data로 선형회귀 모델을 만들 것이다
  
  if count==idx+1:  # 원하는 만큼 뽑아냈으면 break
    break
    
  count+=1  # 행 세주기
f.close()
#print(count)
#print(len(train_data))
#print(np.array(train_data).shape)

f_data=[]
target=[]
#val_data=[]
#val_target=[]

#f_len= int(idx*0.7)
#val_len = int(idx*0.3)
#print(f_len)
#print(val_len)

# train_data
for i in range(count-1):  # 아까 구한 count 를 활용하여 뽑아낸 행의 개수를 구한다
  append_target=[]
  append_target.append(train_data[i][3])  # target data는 더 복잡하게 구하는 이유는 그냥 append 해버리면 이중리스트가 되지 않는다
  target.append(append_target)
  f_data.append(train_data[i][4:])
#print(f'i={i}')  

f_data = np.array(f_data)
target = np.array(target)

def build_model():
    input = Input(shape=(300,), name = 'input')
    hidden1 = Dense(256, activation = 'relu', name = 'dense1')(input)
    hidden2 = Dense(256, activation = 'relu', name = 'dense2')(hidden1)
    hidden3 = Dense(128, activation = 'relu', name = 'dense3')(hidden2)
    output = Dense(1, name = 'output')(hidden3)
    
    model = Model(inputs=[input], outputs=output)
    
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    return model

import ubiquant
env = ubiquant.make_env()   # initialize the environment
iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission

EPOCH=100

kf = KFold(5, shuffle=True, random_state = 111) # random_state = consistency
#oos_y = []
oos_pred = []
oos_pred_1 = []
oos_pred_2 = []

fold = 0
for (test_df, sample_prediction_df) in iter_test:
    numpy_test_df=test_df.to_numpy()
    for train_idx, val_idx in kf.split(f_data):
        fold+=1
        print(f"Fold #{fold}")

        x_train, x_val = f_data[train_idx], f_data[val_idx]
        y_train, y_val = target[train_idx], target[val_idx]

        model = build_model()

        model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs = EPOCH)

        pred = model.predict(numpy_test_df[:,2:].astype(float))

        oos_pred.append(pred)
        print(oos_pred)

for i in fold: 
    oos_pred_1.append(oos_pred[i][0][-1])
    oos_pred_2.append(oos_pred[i][1][-1])
        
oos_pred_1 = np.mean(np.concatenate(oos_pred_1))
oos_pred_2 = np.mean(np.concatenate(oos_pred_2))
sample_prediction_df['target'] = [oos_pred_1, oos_pred_2]
env.predict(sample_prediction_df)   # register your predictions
